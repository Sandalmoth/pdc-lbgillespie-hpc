{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gillespie's algorithm for a logistic branching process where all cells are different\n",
    "\n",
    "The logistic branching process is a stochastic process that obeys logistic growth [[1](https://doi.org/10.1214/105051605000000098)]. It models a population of cells growing exponentially at first, until pairwise competition between cells causes it to reach a steady state (Figure 1). It is described by\n",
    "\n",
    "${dN \\over dt} = \\rho N - dN - cN(N-1)$ .\n",
    "\n",
    "A population of $N$ cells grow at rate $\\rho$ and die at rate $d$ with additional deaths caused by pairwise competition determined by $c(N-1)$. \n",
    "\n",
    "![Example simulation timelines](img/timelines.png)\n",
    "<dl>\n",
    "  <dd style=\"text-align:center\">Figure 1. Example timelines showing the variance inherent in the stochastic model.</dd>\n",
    "</dl>\n",
    "\n",
    "Such a stochastic process can be simulated exactly using the Giellespie algorhithm as follows\n",
    "1. Set up the initial population of cells\n",
    "2. Find the time until the first division as an expontially distributed random variable with $\\lambda = \\sum_{\\text{All cells}} \\rho + d + c(N-1)$.\n",
    "3. Determine what event occured as a random variable with the probability of each outcome being proportional to that event's rate\n",
    "4. Update cell counts, possibly mutating a cell.\n",
    "\n",
    "Assuming all cells are roughly equal, the number of events in a small interval will be proportional to the number of cells $N$.\n",
    "\n",
    "Note that the version implemented in this project is not exactly the formulation described by Lampert; the option of birth-rate being affected by competition is a possibility that might be relevant for evolutionary dynamics so it has been added here. In effect, $c$ is split into two constants $c_\\rho$ and $c_d$, affecting birth rate and death rate respectively. Additionally, if this causes the birth rate to be negative, it overflows into increasing the death rate. This results in the same on average behaviour, as net reproduction rate is identical, but the population size variance and the total number of births/deaths is altered. This model is equivalent if the $c_\\rho = 0$ although further optimizations could be done for this case, since $d$ would never vary with the cell.\n",
    "\n",
    "This would be quick enough to simulate, but I have one further requirement: The birth rate for each cell should depend on it's unique genome. As such, the sum of all rates can no longer be simplified, and calculating all rates also takes time proportional to $N$. The complexity of simulating a set time interval is thus $O(N^2)$, and larger systems take an exorbitant amount of time to simulate.\n",
    "\n",
    "Finally, due to the stochastic nature of these simulations (Figure 1) a single timeline is rarely interesting. Instead, it is neccessary to run several copies (with different random number seeds) to gather statistics. This is embarassingly parallel and easy to do, but if possible I want to try and also speed up the individual calculations by moving them to the GPU. Question is, is the speedup from a GPU large enough, and can enough processes share a single GPU, that there is an actual benefit over simply running more parallel simulations on the CPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm and estimated performance\n",
    "\n",
    "``` python\n",
    "# python-like pseudocode\n",
    "\n",
    "cells = initial_setup(args)\n",
    "\n",
    "while t < t_end:\n",
    "    N = len(cells)\n",
    "    rates = [0.0] * N\n",
    "\n",
    "    # fill vector of rates with\n",
    "    # [birth, birth with mutation, death, birth, birth with mutation, death, birth, ... ]\n",
    "    # calculated for every cell individually\n",
    "    for i, cell in enumerate(cells):\n",
    "        rates[3*i]      = cell.birth_rate() - (N - 1) * cell.birth_interaction();\n",
    "        rates[3*i + 2]  = -min(rates[3*i], 0.0);\n",
    "        rates[3*i]      = max(rates[3*i], 0.0);\n",
    "        rates[3*i + 1]  = rates[3*i] * cell.mutation_rate();\n",
    "        rates[3*i]     -= rates[3*i + 1];\n",
    "        rates[3*i + 2] += cell.get_death_rate() + (N - 1) * cell.death_interaction();\n",
    "\n",
    "    # advance time\n",
    "    event_rate = sum(rates)\n",
    "    t += r_exponential(event_rate) # exponentially distributed R.V.\n",
    "\n",
    "    # select and do event\n",
    "    event = r_select(rates) # discretely distributed R.V. with weights 'rates'\n",
    "    do_event(event)\n",
    "    \n",
    "```\n",
    "##### Scaling with population size for single simulation\n",
    "The time complexity of the three steps are an estimated\n",
    "\n",
    "1. __Fill vector of rates__  \n",
    "   Number of operations proportional to $N$\n",
    "2. __Advance time__  \n",
    "   Number of operations proportional to $N$ (summation)  \n",
    "   Random number generation in constant time\n",
    "3. __Select and do event__  \n",
    "   Select event proportional to $N$ as we have to step through rates until we find a number equally likely to be anywhere  \n",
    "   Do event is constant time\n",
    "   \n",
    "The length of a timestep is proportional to $1/N$ so simulating for a time $\\delta t$ takes a number of steps proportional to $N$.\n",
    "\n",
    "$N$ operations per step for $N$ steps $\\rightarrow O(N^2)$ complexity.\n",
    "\n",
    "The CUDA version has a slightly better event selection performance due to an improved algorithm (appendix 1). This might lead to a slightly better scaling with population size.\n",
    "\n",
    "##### Scaling with parallelism for multiple simulations\n",
    "No communication between threads at all, I expect near perfect scaling for the CPU version with number of cores. A slight loss could be possible from the very simplistic output printing with all threads printing to std::out. However, for large population sizes the amount of output is small compared to the compute workload (about one line of printing per second) so that effect should diminish with large $N$.\n",
    "\n",
    "For the CUDA version, most of the computation time is on the GPU. Therefore, I expect a small amount of scaling with number of parallel simulations, as having more than one should improve the GPU usage since it never has to wait. However, for small population sizes, maybe scheduling so many small kernels could decrease performance.\n",
    "\n",
    "In conclusion then, for $m$ simulations of $N$ cells on $k$ cores I expect  \n",
    "$\\text{time} = N^2 {m \\over k}$  \n",
    "for the sequential code and  \n",
    "$\\text{time} = N^2 m$  \n",
    "for the CUDA code (with the finer points above)\n",
    "\n",
    "Since each individual simulation should be much faster on the GPU, the fastest version in actual time taken could be either one, depending on the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "The sequential simulator was rewritten in CUDA and optimized for performance with large cell counts (>100 000). See Appendix 1 for details. For both the sequential and CUDA simulators, the main function was then modified to run a series of simulations in parallel using an openMP parallel for construct. In the CUDA version each simulator object was also assigned its own CUDA stream to improve work-sharing on the GPU. Source code is available on [github](https://github.com/Sandalmoth/pdc-lbgillespie-hpc/branches).\n",
    "\n",
    "Attempted verification of the ouput suggested that something was wrong as timelines from sequential and CUDA versions sometimes differed despite identical initial conditions. However, these discrepancies were tracked down to numerical inaccuracies (Appendix 2).\n",
    "\n",
    "Both simulators were benchmarked running 64 simulations using between 1 and 16 threads in parallel for three different population sizes (1400, 14000 and 140000) on a Xeon E5-2630 with a Quadro K620 accelerator (Figure 2-4). The length (in simulation time) for each of the population sizes was adjusted such that benchmarking wouldn't take excessively long, but also to be long enough to lower the relative overhead from parsing input, creating openMP threads and copying the cell population to the GPU in the first timestep (Table 1). For production runs, the time taken by these factors is insignificantly small.\n",
    "\n",
    "\n",
    "| Population size | Simulation time |\n",
    "| --------------- | --------------- |\n",
    "|   1 400         | 400             |\n",
    "|  14 000         | 20              |\n",
    "| 140 000         | 0.6             |\n",
    "<dl>\n",
    "  <dd style=\"text-align:center\">Table 1. Benchmarking parameters.</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark results\n",
    "![Summation method](img/summation2.png)\n",
    "<dl>\n",
    "  <dd style=\"text-align:center\">Figure 4. Simulation time.</dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### Future possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Appendix 1: Details of CUDA optimization\n",
    "\n",
    "There are three main alterations in the CUDA version of the program.\n",
    "1. The calculations of rates is done on the GPU.\n",
    "2. The summation of rates is done on the GPU in multiple nested steps.\n",
    "3. The vectors of partial sums from the nested summation is used to speed up event selection.\n",
    "\n",
    "In principle, points 2 and 3 could be applied to a sequential program as well. However, any attempts at implementation resulted in overall slower code. Nevertheless, it is a potential optimization with the right clever programming.\n",
    "\n",
    "#### Calculation of rates\n",
    "The calculation loop was changed into a kernel. Since all cells rates are independent the translation is very straightforward. This is significantly faster for large $N$, but has the same scaling as the sequential algorithm.\n",
    "\n",
    "#### Summation of rates\n",
    "![Summation method](img/summation2.png)\n",
    "<dl>\n",
    "  <dd style=\"text-align:center\">Figure 5. Schema for iterative block sums.</dd>\n",
    "</dl>\n",
    "\n",
    "Rather than summing all the rates at once, blocks of SUM_BLOCK_SIZE elements are summed in stored in a new vector. The process is then repeated SUM_DEPTH times summing these block level sums with the same algorithm. Only the deepest level of block sums is transferred to the CPU and summed locally. While technically this doesn't lower the number of summmation operations the partial sums are very useful in the next step.\n",
    "\n",
    "#### Event selection\n",
    "![Selection method](img/selection.png)\n",
    "<dl>\n",
    "  <dd style=\"text-align:center\">Figure 6. Schema for rapid event selection using block sums.</dd>\n",
    "</dl>\n",
    "By having access to the partial sums from before, it is possible to do the event selection in better that $O(N)$ time. First the deepest level of sums (that were transferred to the host anyway) is traversed, looking for the segment where the event occured. That single segment is then fetched from the next level of partial sums, and so on until finally a small block of the rates themselves is fetched and the rate is found. This both lowers the memory transfer and the number of values we have to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 2: On verification and reproducibility\n",
    "\n",
    "While I originally intended the sequential and CUDA versions of the program to give exactly the same result if started with the same random number seeds, the requirement was dropped. The reason lies in very small discrepancies in the floating point math that, when the simulations get large enough, cause them to gradually diverge.\n",
    "\n",
    "![Divergence](img/divergence.png)\n",
    "<dl>\n",
    "  <dd style=\"text-align:center\">Figure 7. Simulations often diverge at large $N$.</dd>\n",
    "</dl>\n",
    "\n",
    "First, in the calculation of the death rate we have the operation:\n",
    "```C++\n",
    "rates[3*i + 2] += cells[i].get_death_rate() + (cells.size() - 1) * cells[i].get_death_interaction();\n",
    "\n",
    "```\n",
    "(slightly different in CUDA)\n",
    "This appears to be using ordinary +, - and * operators on the CPU, whereas it seems the GPU uses a fused multiply-add operation [[3](https://docs.nvidia.com/cuda/floating-point/index.html)]. The latter has only a single rounding, which in rare cases produces a very slightly different answer.\n",
    "\n",
    "Second, in the summation of rates. Summing floating point values on a computer is not distributive, and changing up the order or methodology changes the answer somewhat. Optimally, something like Kahan's summation algorithm or similar [[2](http://www.phys.uconn.edu/~rozman/Courses/P2200_11F/downloads/sum-howto.pdf)] could be used to minimized the errors. Both that and verifying that the summation method causes no bias is beyond the scope of this project however.\n",
    "\n",
    "Neither of these discrepancies constitute a genuine error (probably), but they are simply different variants on numerical artefacts. So long as the on-average behaviour of the simulations remain correct and each individual trajectory is valid, these differences are not important. It does mean however that a perfectly fair comparison between sequential and CUDA code is more difficult.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. Lambert, A. _The branching process with logistic growth_ https://doi.org/10.1214/105051605000000098\n",
    "2. Higham, N. _How and How Not to Sum Floating Point Numbers_ http://www.phys.uconn.edu/~rozman/Courses/P2200_11F/downloads/sum-howto.pdf\n",
    "3. _Floating Point and IEEE 754 Compliance for NVIDIA GPUs_ https://docs.nvidia.com/cuda/floating-point/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
